{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Mining with Sklearn\n",
    "\n",
    "\n",
    "\n",
    "__Author:__ Colin Cambo\n",
    "\n",
    "__Date:__ 4/18/2017\n",
    "\n",
    "__About:__ This notebook covers the basics of text mining with sklearn.\n",
    "\n",
    "__Contents:__\n",
    "\n",
    "  * [0 Introduction 20 newsgroups Dataset](#0)\n",
    "    * [0.1 Loading Data](#0.1)\n",
    "    * [0.2 Simple Exploration](#0.2)\n",
    "  * [1 Text Mining Basics](#1)\n",
    "    * [1.1 CountVectorizer](#1.1)\n",
    "        * [1.1.1 Description](#1.1.1)\n",
    "        * [1.1.2 Example](#1.1.2)\n",
    "        * [1.1.3 Parameters](#1.1.3)\n",
    "          * [1.1.3.1 min_df](#1.1.3.1)\n",
    "          * [1.1.3.2 max_df](#1.1.3.2)\n",
    "          * [1.1.3.3 ngram_range](#1.1.3.3)\n",
    "          * [1.1.3.4 max_features](#1.1.3.4)\n",
    "    * [1.2 TfidfTransformer](#1.2)\n",
    "      * [1.2.1 Description](#1.2.1)\n",
    "      * [1.2.2 Example](#1.2.2)\n",
    "      * [1.2.3 Parameters](#1.2.3)\n",
    "          * [1.2.3.1 norm](#1.2.3.1)\n",
    "          * [1.2.3.2 use_idf](#1.2.3.2)\n",
    "  * [2 Modeling](#2)\n",
    "    * [2.1 Multinomial Naive Bayes](#2.1)\n",
    "      * [2.1.1 Description](#2.1.1)\n",
    "      * [2.1.2 Example](#2.1.2)\n",
    "      * [2.1.3 Parameters](#2.1.3)\n",
    "        * [2.1.3.1 alpha](#2.1.3.1)\n",
    "    * [2.2 Stochastic Gradient Descent](#2.2)\n",
    "      * [2.2.1 Description](#2.2.1)\n",
    "      * [2.2.2 Example](#2.2.2)\n",
    "      * [2.2.3 Parameters](#2.2.3)\n",
    "        * [2.2.3.1 alpha](#2.2.3.1)\n",
    "        * [2.2.3.2 epsilon](#2.2.3.2)\n",
    "  * [3 Tuning Models](#3)\n",
    "    * [3.1 Pipelines](#3.1)\n",
    "      * [3.1.1 Description](#3.1.1)\n",
    "      * [3.1.2 Example](#3.1.2)\n",
    "    * [3.2 Parameter Seach](#3.2)\n",
    "      * [3.2.1 GridSearchCV](#3.2.1)\n",
    "        * [3.2.1.1 Description](#3.2.1)\n",
    "        * [3.2.1.2 Example](#3.2.2)\n",
    "        * [3.2.1.3 Parameters](#3.2.3)\n",
    "      * [3.2.1 RandomizedSearchCV](#3.2.1)\n",
    "        * [3.2.1.1 Description](#3.2.1)\n",
    "        * [3.2.1.2 Example](#3.2.2)\n",
    "        * [3.2.1.3 Parameters](#3.2.3)\n",
    "  * [4 Increasing Performance](#4)\n",
    "    * [4.1 Stemming](#4.1)\n",
    "      * [4.1.1 Description](#4.1.1)\n",
    "      * [4.1.2 Example](#4.1.2)\n",
    "      * [4.1.3 Types](#4.1.3)\n",
    "        * [2.2.3.1 Porter](#4.1.3.1)\n",
    "        * [2.2.3.1 Lancaster](#4.1.3.2)\n",
    "        * [4.1.3.3 Snowball](#4.1.3.3)\n",
    "    * [4.2 Custom Transformers](#4.2)\n",
    "      * [4.2.1 ColSelector](#4.2.1)\n",
    "        \n",
    "__Requirements__\n",
    "\n",
    "Run the cell below to pip install all dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import f1_score, accuracy_score, log_loss\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from ipywidgets import interact\n",
    "from time import time\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='0'></a>\n",
    "# 0 Introduction 20 Newsgroups Dataset\n",
    "\n",
    "The 20 newsgroups dataset comprises around 18000 newsgroups posts on 20 topics split in two subsets: one for training (or development) and the other one for testing (or for performance evaluation). The split between the train and test set is based upon a messages posted before and after a specific date.\n",
    "\n",
    "For more information about the data: http://qwone.com/~jason/20Newsgroups/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='0.1'></a>\n",
    "## 0.1 Loading Data\n",
    "\n",
    "We will use sklearn to load the data, so we will import the function fetch_20newsgroups from sklearn.datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The default parameters will return the entire dataset as a python object. Below we will set data equal to the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = fetch_20newsgroups()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DESCR', 'data', 'description', 'filenames', 'target', 'target_names']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#prints all objects inside train\n",
    "dir(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train.description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Target names are stored in data.target_names, below are all of the target names currently in our dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data.target_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Counter we can see just how many observations belong to each of our classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Counter(data.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall the dataset is pretty balanced between the classes, which makes classification easier for us, but there are still too many observations for us to effectively model in a short period, so we will subset the dataset by taking only 5 classes.\n",
    "\n",
    "To subset the dataset by classes you just have to pass a list of the classes you're interested in to the fetch_20newsgroups function's categories variable. We will also subset our dataset to just get the train set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cat = ['rec.sport.baseball', 'sci.electronics', 'talk.politics.misc', 'sci.crypt', 'misc.forsale']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = fetch_20newsgroups(subset='train', categories=cat, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='0.2'></a>\n",
    "## 0.2 Simple Data Exploration\n",
    "\n",
    "This lecture is for text mining so our data exploration will be minimal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(train.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(train.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train.target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Counter(train.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'Text':train.data, 'Target':train.target})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(df.iloc[1]['Text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This isn't your usual text data, so we will see if this still follows Zipf's law\n",
    "\n",
    "__[Zipf's Law](https://en.wikipedia.org/wiki/Zipf%27s_law):__  Zipf's law states that given some corpus of natural language utterances, the frequency of any word is inversely proportional to its rank in the frequency table. Thus the most frequent word will occur approximately twice as often as the second most frequent word, three times as often as the third most frequent word, etc.: the rank-frequency distribution is an inverse relation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "import re\n",
    "token_regex = re.compile(r'\\b\\w\\w+\\b')\n",
    "\n",
    "#Here we're just combining all tokens into one massive string\n",
    "all_msgs = reduce((lambda x, y: x + y), map(lambda x: token_regex.findall(x.lower()), df['Text'].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "count = Counter(all_msgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Sorting counts\n",
    "word_counts = sorted(list(count.items()), key=lambda x:x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Displaying first 10\n",
    "word_counts[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "subset_word_counts = word_counts[:1000]\n",
    "plt.figure(figsize=(8,8))\n",
    "plt.scatter(x=range(len(subset_word_counts)), y=[w[1] for w in subset_word_counts])\n",
    "plt.title('Count for Words')\n",
    "plt.ylabel('Count')\n",
    "plt.xlabel('Word index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "subset_word_counts = word_counts[:1000]\n",
    "plt.figure(figsize=(8,8))\n",
    "plt.scatter(x=range(len(subset_word_counts)), y=[np.log(w[1]) for w in subset_word_counts])\n",
    "plt.title('log of Count for Words')\n",
    "plt.ylabel('log(Count)')\n",
    "plt.xlabel('Word index')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yup that's Zipf's law"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='1'></a>\n",
    "# 1 sklearn Text Mining Basics\n",
    "\n",
    "sklearn has many great tools for text mining, in this section we will cover two of the more important ones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='1.1'></a>\n",
    "## 1.1 CountVectorizer\n",
    "\n",
    "In order to perform machine learning on text documents, we first need to turn the text content into numerical feature vectors. This is typically done with __Bag-of-Words__, and this is where the CountVectorizer class comes in handy. You can read more about the CountVectorizer [here](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='1.1.1'></a>\n",
    "### 1.1.1 Description\n",
    "\n",
    "__FROM SKLEARN [DOCUMENTATION](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html)__\n",
    "\n",
    "Convert a collection of text documents to a matrix of token counts\n",
    "\n",
    "This implementation produces a sparse representation of the counts using scipy.sparse.coo_matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='1.1.2'></a>\n",
    "### 1.1.2 Example\n",
    "\n",
    "To get a better understanding of the class we will do an example below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text = ['cat dog dog dog dog dog dog dog', 'cat fish puppy', 'fish cat']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vect = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#vect.fit(text)\n",
    "#dtm = vect.transform(text)\n",
    "dtm = vect.fit_transform(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dtm.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vect.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df2 = pd.DataFrame(dtm.toarray(), columns=vect.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dtm_todf(dtm, vect):\n",
    "    return pd.DataFrame(dtm.toarray(), columns=vect.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dtm_todf(dtm, vect)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__EXERCISE:__\n",
    "\n",
    "  * 1) Transform new_str below into a document term matrix with our fitted CountVectorizer. Does the output look like you thought?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_str = ['kitten puPPy fish', \"Fish dog's\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# %load exercise1.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='1.1.3'></a>\n",
    "### 1.1.3 Parameters\n",
    "\n",
    "This class has many parameters but we will highlight a few of the most important ones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='1.1.3.1'></a>\n",
    "#### 1.1.3.1 min_df\n",
    "\n",
    "__min_df__ : float in range [0.0, 1.0] or int, default=1\n",
    "\n",
    "When building the vocabulary ignore terms that have a document frequency strictly lower than the given threshold. This value is also called cut-off in the literature. If float, the parameter represents a proportion of documents, integer absolute counts. This parameter is ignored if vocabulary is not None."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Example__\n",
    "\n",
    "We will show what the document term matrix looks like before and after min_df parameter is used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cv = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_dtm = cv.fit_transform(df['Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_dtm.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the shape of our dtm is (2833, 38850) before using min_df.\n",
    "\n",
    "Lets see what the shape is after using a min_df of 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cv = CountVectorizer(min_df=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_dtm = cv.fit_transform(df['Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_dtm.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Too see how the min_df affects the dtm's shape for different values we will number of columns below for a min_df from 1 to 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "start = 1\n",
    "end = 10\n",
    "plt.figure(figsize=(8,8))\n",
    "plt.title('Columns vs min_df')\n",
    "plt.xlabel('min_df')\n",
    "plt.ylabel('Column Number')\n",
    "for min_df in np.arange(start,end,1):\n",
    "    cv = CountVectorizer(min_df=min_df)\n",
    "    train_dtm = cv.fit_transform(df['Text'])\n",
    "    num_cols = train_dtm.shape[1]\n",
    "    print('min_df={} has shape {}'.format(min_df, train_dtm.shape))\n",
    "    plt.scatter(min_df, num_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='1.1.3.2'></a>\n",
    "#### 1.1.3.2 max_df\n",
    "\n",
    "__max_df :__ float in range [0.0, 1.0] or int, default=1.0\n",
    "\n",
    "When building the vocabulary ignore terms that have a document frequency strictly higher than the given threshold (corpus-specific stop words). If float, the parameter represents a proportion of documents, integer absolute counts. This parameter is ignored if vocabulary is not None."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Example__\n",
    "\n",
    "We will show what max_df does to the document term matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cv = CountVectorizer()\n",
    "train_dtm = cv.fit_transform(df['Text'])\n",
    "print(\"The shape of the dtm with default max_df: {}\".format(train_dtm.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cv = CountVectorizer(max_df=.5)\n",
    "train_dtm = cv.fit_transform(df['Text'])\n",
    "print(\"The shape of the dtm with .5 max_df: {}\".format(train_dtm.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To view the impact of max_df for different values we will plot the feature space for a range of max_df values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,8))\n",
    "plt.ylabel('Number of Columns')\n",
    "plt.xlabel('Value of max_df')\n",
    "plt.title('Columns vs max_df')\n",
    "for max_df in np.arange(.1,1.0,.1):\n",
    "    cv = CountVectorizer(max_df=max_df)\n",
    "    train_dtm = cv.fit_transform(df['Text'])\n",
    "    num_cols = train_dtm.shape[1]\n",
    "    print('max_df={:f} has shape {}'.format(max_df, train_dtm.shape))\n",
    "    plt.scatter(max_df, num_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,8))\n",
    "plt.title('Columns vs max_df')\n",
    "plt.xlabel('max_df')\n",
    "plt.ylabel('Number Columns')\n",
    "for max_df in np.arange(25,200,25):\n",
    "    cv = CountVectorizer(max_df=max_df)\n",
    "    train_dtm = cv.fit_transform(df['Text'])\n",
    "    num_cols = train_dtm.shape[1]\n",
    "    print('max_df={} has shape {}'.format(max_df, train_dtm.shape))\n",
    "    plt.scatter(max_df, num_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "__Exercise:__\n",
    "\n",
    "  * 1) Find a max_df value that will return half the number of columns (19,000)\n",
    "  * 2) Find a min_df value that will return half the number of columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %load exercise2.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='1.1.3.3'></a>\n",
    "#### 1.1.3.3 ngram_range\n",
    "\n",
    "__ngram_range :__ tuple (min_n, max_n)\n",
    "\n",
    "The lower and upper boundary of the range of n-values for different n-grams to be extracted. All values of n such that min_n <= n <= max_n will be used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Example__\n",
    "\n",
    "We will show what happens to the document term matrix when different combinations are used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cv = CountVectorizer(ngram_range=(1,1))\n",
    "train_dtm = cv.fit_transform(df['Text'])\n",
    "print(\"The shape of the dtm with ngram_range=(1,1): {}\".format(train_dtm.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cv = CountVectorizer(ngram_range=(1,2))\n",
    "train_dtm = cv.fit_transform(df['Text'])\n",
    "print(\"The shape of the dtm with ngram_range=(1,2): {}\".format(train_dtm.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text = ['cat dog dog dog dog dog dog dog', 'cat fish puppy', 'fish cat']\n",
    "cv = CountVectorizer(ngram_range=(1,2))\n",
    "test_dtm = cv.fit_transform(text)\n",
    "dtm_todf(test_dtm, cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='1.1.3.4'></a>\n",
    "#### 1.1.3.4 stop_words\n",
    "\n",
    "__stop_words :__ string {‘english’}, list, or None (default)\n",
    "\n",
    "If ‘english’, a built-in stop word list for English is used.\n",
    "If a list, that list is assumed to contain stop words, all of which will be removed from the resulting tokens. Only applies if analyzer == 'word'.\n",
    "\n",
    "If None, no stop words will be used. max_df can be set to a value in the range [0.7, 1.0) to automatically detect and filter stop words based on intra corpus document frequency of terms.\n",
    "\n",
    "__Example__\n",
    "\n",
    "We will show how removing stop_words affects our document term matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cv = CountVectorizer(stop_words=None)\n",
    "train_dtm = cv.fit_transform(df['Text'])\n",
    "print(\"The shape of the dtm with stop_words=None: {}\".format(train_dtm.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cv = CountVectorizer(stop_words='english')\n",
    "train_dtm = cv.fit_transform(df['Text'])\n",
    "print(\"The shape of the dtm with stop_words=None: {}\".format(train_dtm.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='1.1.3.5'></a>\n",
    "#### 1.1.3.5 max_features\n",
    "\n",
    "__max_features :__ int or None, default=None\n",
    "\n",
    "If not None, build a vocabulary that only consider the top max_features ordered by term frequency across the corpus.\n",
    "\n",
    "This parameter is ignored if vocabulary is not None.\n",
    "\n",
    "__Example__\n",
    "\n",
    "We will show how max_features affects our dtm, and how it interacts with other CountVectorizer parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cv = CountVectorizer(max_features=20)\n",
    "train_dtm = cv.fit_transform(df['Text'])\n",
    "dtm_todf(train_dtm, cv).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cv = CountVectorizer(max_features=20, stop_words='english')\n",
    "train_dtm = cv.fit_transform(df['Text'])\n",
    "dtm_todf(train_dtm, cv).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How can we get rid of these?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "__Exercise:__\n",
    "\n",
    "  * 1) Find the 20 most common bi-grams in our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#%load exercise3.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='1.2'></a>\n",
    "## 1.2 TfidfTransformer\n",
    "\n",
    "Counting occurences is a good start but there is still the issue of longer documents having higher average count values than shorter documents, even though they might talk about the same topics.\n",
    "\n",
    "The way to get around this issue is to divide the number of occurences of each word in each document by the total number of words in the document. These new features are called term frequencies.\n",
    "\n",
    "Another common addition to term frequencies is to downscale weights for words that will occur in many documents in the corpus. This will help us in our models because we are making the assumption that these common words are less informative than the words that are less frequent.\n",
    "\n",
    "This downscaling is called tf–idf for “Term Frequency times Inverse Document Frequency”.\n",
    "\n",
    "This process is implemented by the __TfidfTransformer__ in sklearn, to read more about it click [here](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='1.2.1'></a>\n",
    "### 1.2.1 Description\n",
    "\n",
    "__FROM SKLEARN [DOCUMENTATION](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html)__\n",
    "\n",
    "Transform a count matrix to a normalized tf or tf-idf representation\n",
    "\n",
    "Tf means term-frequency while tf-idf means term-frequency times inverse document-frequency. This is a common term weighting scheme in information retrieval, that has also found good use in document classification.\n",
    "\n",
    "The goal of using tf-idf instead of the raw frequencies of occurrence of a token in a given document is to scale down the impact of tokens that occur very frequently in a given corpus and that are hence empirically less informative than features that occur in a small fraction of the training corpus.\n",
    "\n",
    "The formula that is used to compute the tf-idf of term t is tf-idf(d, t) = tf(t) * idf(d, t), and the idf is computed as idf(d, t) = log [ n / df(d, t) ] + 1 (if smooth_idf=False), where n is the total number of documents and df(d, t) is the document frequency; the document frequency is the number of documents d that contain term t. The effect of adding “1” to the idf in the equation above is that terms with zero idf, i.e., terms that occur in all documents in a training set, will not be entirely ignored. (Note that the idf formula above differs from the standard textbook notation that defines the idf as idf(d, t) = log [ n / (df(d, t) + 1) ]).\n",
    "\n",
    "If smooth_idf=True (the default), the constant “1” is added to the numerator and denominator of the idf as if an extra document was seen containing every term in the collection exactly once, which prevents zero divisions: idf(d, t) = log [ (1 + n) / 1 + df(d, t) ] + 1.\n",
    "\n",
    "Furthermore, the formulas used to compute tf and idf depend on parameter settings that correspond to the SMART notation used in IR as follows:\n",
    "\n",
    "Tf is “n” (natural) by default, “l” (logarithmic) when sublinear_tf=True. Idf is “t” when use_idf is given, “n” (none) otherwise. Normalization is “c” (cosine) when norm='l2', “n” (none) when norm=None."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='1.2.2'></a>\n",
    "### 1.2.2 Example\n",
    "\n",
    "To get a better understanding of the class we will do an example below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text = ['cat dog dog dog dog dog dog dog', 'cat fish puppy', 'fish cat']\n",
    "vect = CountVectorizer()\n",
    "dtm = vect.fit_transform(text)\n",
    "example_df = dtm_todf(dtm, vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "example_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfidf = TfidfTransformer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dtm = tfidf.fit_transform(example_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dtm_todf(dtm, vect)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These results are hard to explain without first looking at each default parameter individually"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='1.2.3'></a>\n",
    "### 1.2.3 Parameters\n",
    "\n",
    "This class has much less parameters than CountVectorizer but these parameters aren't as intuitive so we'll cover them below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='1.2.3.1'></a>\n",
    "#### 1.2.3.1 norm\n",
    "\n",
    "__norm :__ ‘l1’, ‘l2’ or None, optional\n",
    "\n",
    "Norm used to normalize term vectors. None for no normalization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Example__\n",
    "\n",
    "We will compare what each norm does to our document term matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "example_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We haven't covered it yet but the use_idf parameter can be set to false so only term frequencies are done. We will set it equal to false for this example because it will really help to highlight what the norm parameter is doing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__norm = 'l1'__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfidf = TfidfTransformer(norm='l1', use_idf=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dtm = tfidf.fit_transform(example_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dtm_todf(dtm, vect)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__norm = 'l2'__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfidf = TfidfTransformer(norm='l2', use_idf=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dtm = tfidf.fit_transform(example_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dtm_todf(dtm, vect)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What's the l2 norm doing to our document term matrix?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mat = example_df.as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "[np.sqrt(x**2/sum(x**2)) for x in mat]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__norm = None__\n",
    "\n",
    "This is perhaps the least exciting norm, but it is still very important"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfidf = TfidfTransformer(norm=None, use_idf=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dtm = tfidf.fit_transform(example_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dtm_todf(dtm, vect)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='1.2.3.2'></a>\n",
    "#### 1.2.3.2 use_idf\n",
    "\n",
    "__use_idf :__ boolean, default=True\n",
    "\n",
    "Enable inverse-document-frequency reweighting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Example__\n",
    "\n",
    "We will show how use_idf=True will interact with the different norms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "norm_options = ['l1', 'l2', 'None']\n",
    "@interact\n",
    "def try_norm(norm=norm_options):\n",
    "    if norm == 'None':\n",
    "        norm = None\n",
    "    tfidf = TfidfTransformer(norm=norm, use_idf=True)\n",
    "    dtm = tfidf.fit_transform(example_df)\n",
    "    print(dtm_todf(dtm, vect))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='2'></a>\n",
    "# 2 Modeling\n",
    "\n",
    "Now that we learned the basics we can begin the modeling process. There are many different algorithms and techniques but I will only cover two of the ones we tried in our practicum project.\n",
    "\n",
    "We will also split our data up here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df['Text'], df['Target'], test_size=.2, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='2.1'></a>\n",
    "## 2.1 MultinomialNB\n",
    "\n",
    "Multinomial Naive Bayes is known as a good dependable baseline for text classification\n",
    "\n",
    "__Advantages:__ \n",
    "\n",
    "  * Very fast\n",
    "  * Robust to irrelevant features\n",
    "  * Works well with little data\n",
    "  * Very hard to overfit due to simplicity\n",
    "\n",
    "__Disadvantages:__\n",
    "\n",
    "  * Will most likely underfit if independence assumption doesn't hold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='2.1.1'></a>\n",
    "### 2.1.1 Description\n",
    "\n",
    "__FROM SKLEARN [DOCUMENTATION](http://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html)__\n",
    "\n",
    "Naive Bayes classifier for multinomial models\n",
    "\n",
    "The multinomial Naive Bayes classifier is suitable for classification with discrete features (e.g., word counts for text classification). The multinomial distribution normally requires integer feature counts. However, in practice, fractional counts such as tf-idf may also work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='2.1.2'></a>\n",
    "### 2.1.2 Example\n",
    "\n",
    "We will begin the modeling process with the actual dataset now"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will just use the default parameters and see what they give us.\n",
    "\n",
    "__NOTE:__ Make sure you're not fitting CountVectorizer or TfidfTransformer with test data! This is cheating and no one likes a cheater!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__FITTING TRAIN SET__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cv = CountVectorizer()\n",
    "clf = MultinomialNB()\n",
    "\n",
    "X_train_dtm = cv.fit_transform(X_train)\n",
    "clf.fit(X_train_dtm, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__PREDICTING TEST SET__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_test_dtm = cv.transform(X_test)\n",
    "y_pred = clf.predict(X_test_dtm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__EVALUATING PREDICTIONS__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def evaluate_model(actual, prediction):\n",
    "    \n",
    "    print(\"Classification report: \\n{}\".format(classification_report(actual, prediction)))\n",
    "    print('Confusion Matrix: \\n{}'.format(confusion_matrix(actual, prediction)))\n",
    "    print('\\nAccuracy: {}'.format(accuracy_score(actual, prediction)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "evaluate_model(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "idf_options = ['False', 'True']\n",
    "norm_options = ['l1', 'l2', 'None']\n",
    "@interact\n",
    "def try_alpha(use_idf=idf_options, norm=norm_options):\n",
    "    clf = MultinomialNB()\n",
    "    \n",
    "    if norm == 'None':\n",
    "        norm = None\n",
    "    if use_idf == 'False':\n",
    "        use_idf = False\n",
    "    else:\n",
    "        use_idf = True\n",
    "    tfidf = TfidfTransformer(use_idf=use_idf, norm=norm)\n",
    "    \n",
    "    new_train_dtm = tfidf.fit_transform(X_train_dtm)\n",
    "    new_test_dtm = tfidf.transform(X_test_dtm)\n",
    "    \n",
    "    clf.fit(new_train_dtm, y_train)\n",
    "    y_pred = clf.predict(new_test_dtm)\n",
    "    evaluate_model(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='2.1.3'></a>\n",
    "### 2.1.3 Parameters\n",
    "\n",
    "One of the benefits of multinomial naive bayes is that there are very little parameters to tune"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<a id='2.1.3.1'></a>\n",
    "#### 2.1.3.1 alpha\n",
    "\n",
    "__alpha :__ float, optional (default=1.0)\n",
    "\n",
    "Additive (Laplace/Lidstone) smoothing parameter (0 for no smoothing).\n",
    "\n",
    "__Example__\n",
    "\n",
    "We will show what a different alpha value does to coefficients and prediction power\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cv = CountVectorizer(max_features=1000)\n",
    "X_train_dtm = cv.fit_transform(X_train)\n",
    "X_test_dtm = cv.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "alphas = [0, .00001, .0001, .0001, .001, .01, .1, 1]\n",
    "idf_options = [False, True]\n",
    "norm_options = ['l1', 'l2', None]\n",
    "@interact\n",
    "def try_alpha(alpha=alphas):\n",
    "    clf = MultinomialNB(alpha=float(alpha))\n",
    "    clf.fit(X_train_dtm, y_train)\n",
    "    y_pred = clf.predict(X_test_dtm)\n",
    "    evaluate_model(y_test, y_pred)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='2.2'></a>\n",
    "## 2.2 SGDClassifier\n",
    "\n",
    "__FROM SKLEARN [DOCUMENTATION](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html#sklearn.linear_model.SGDClassifier):__\n",
    "\n",
    "Linear classifiers (SVM, logistic regression, a.o.) with SGD training.\n",
    "\n",
    "This estimator implements regularized linear models with stochastic gradient descent (SGD) learning: the gradient of the loss is estimated each sample at a time and the model is updated along the way with a decreasing strength schedule (aka learning rate). SGD allows minibatch (online/out-of-core) learning, see the partial_fit method. For best results using the default learning rate schedule, the data should have zero mean and unit variance.\n",
    "\n",
    "This implementation works with data represented as dense or sparse arrays of floating point values for the features. The model it fits can be controlled with the loss parameter; by default, it fits a linear support vector machine (SVM).\n",
    "\n",
    "The regularizer is a penalty added to the loss function that shrinks model parameters towards the zero vector using either the squared euclidean norm L2 or the absolute norm L1 or a combination of both (Elastic Net). If the parameter update crosses the 0.0 value because of the regularizer, the update is truncated to 0.0 to allow for learning sparse models and achieve online feature selection.\n",
    "\n",
    "\n",
    "<a id='2.2.1'></a>\n",
    "### 2.2.1 Description\n",
    "\n",
    "__FROM SKLEARN [DOCUMENTATION](http://scikit-learn.org/stable/modules/sgd.html):__\n",
    "\n",
    "Stochastic Gradient Descent (SGD) is a simple yet very efficient approach to discriminative learning of linear classifiers under convex loss functions such as (linear) Support Vector Machines and Logistic Regression. Even though SGD has been around in the machine learning community for a long time, it has received a considerable amount of attention just recently in the context of large-scale learning.\n",
    "\n",
    "SGD has been successfully applied to large-scale and sparse machine learning problems often encountered in text classification and natural language processing. Given that the data is sparse, the classifiers in this module easily scale to problems with more than 10^5 training examples and more than 10^5 features.\n",
    "\n",
    "The __advantages__ of Stochastic Gradient Descent are:\n",
    "\n",
    "  * Efficiency.\n",
    "  * Ease of implementation (lots of opportunities for code tuning).\n",
    "\n",
    "The __disadvantages__ of Stochastic Gradient Descent include:\n",
    "\n",
    "  * SGD requires a number of hyperparameters such as the regularization parameter and the number of iterations.\n",
    "  * SGD is sensitive to feature scaling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='2.2.2'></a>\n",
    "### 2.2.2 Example\n",
    "\n",
    "We will try SGD with the default parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__FITTING THE TRAINING SET__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cv = CountVectorizer()\n",
    "clf = SGDClassifier(random_state=0)\n",
    "\n",
    "X_train_dtm = cv.fit_transform(X_train)\n",
    "clf.fit(X_train_dtm, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__PREDICTING TEST__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_test_dtm = cv.transform(X_test)\n",
    "y_pred = clf.predict(X_test_dtm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__EVALUATING MODEL__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "evaluate_model(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting, why didn't this model perform better than multinomial naive bayes?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__EXERCISE:__\n",
    "\n",
    "  * 1) Re-run the model with TfidfTransformer. Are the results better?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#%load exercise4.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='2.2.3'></a>\n",
    "### 2.2.3 Parameters\n",
    "\n",
    "SGDClassifier has many parameters, we'll examine a few of the more important ones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='2.2.3.1'></a>\n",
    "#### 2.2.3.1 loss\n",
    "\n",
    "__loss :__ str, ‘hinge’, ‘log’, ‘modified_huber’, ‘squared_hinge’, ‘perceptron’, or a regression loss: ‘squared_loss’, ‘huber’, ‘epsilon_insensitive’, or ‘squared_epsilon_insensitive’\n",
    "\n",
    "The loss function to be used. Defaults to ‘hinge’, which gives a linear SVM. The ‘log’ loss gives logistic regression, a probabilistic classifier. ‘modified_huber’ is another smooth loss that brings tolerance to outliers as well as probability estimates. ‘squared_hinge’ is like hinge but is quadratically penalized. ‘perceptron’ is the linear loss used by the perceptron algorithm. The other losses are designed for regression but can be useful in classification as well; see SGDRegressor for a description."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Example__\n",
    "\n",
    "We will try a few different loss functions to see their effect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "loss_options = ['log', 'hinge', 'modified_huber', 'squared_hinge', 'perceptron']\n",
    "cv = CountVectorizer()\n",
    "X_train_dtm = cv.fit_transform(X_train)\n",
    "X_test_dtm = cv.transform(X_test)\n",
    "\n",
    "tfidf = TfidfTransformer()\n",
    "X_train_dtm = tfidf.fit_transform(X_train_dtm)\n",
    "X_test_dtm = tfidf.transform(X_test_dtm)\n",
    "@interact\n",
    "def try_loss_functions(loss=loss_options):\n",
    "    \n",
    "    clf = SGDClassifier(loss=loss, random_state=0)\n",
    "    clf.fit(X_train_dtm, y_train)\n",
    "    y_pred = clf.predict(X_test_dtm)\n",
    "    evaluate_model(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='2.2.3.2'></a>\n",
    "#### 2.2.3.2 penalty\n",
    "\n",
    "__penalty :__ str, ‘none’, ‘l2’, ‘l1’, or ‘elasticnet’\n",
    "\n",
    "The penalty (aka regularization term) to be used. Defaults to ‘l2’ which is the standard regularizer for linear SVM models. ‘l1’ and ‘elasticnet’ might bring sparsity to the model (feature selection) not achievable with ‘l2’."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "penalty_options = ['l1', 'l2', 'elasticnet']\n",
    "alpha_options = [.000001, .00001, .0001, .001]\n",
    "cv = CountVectorizer()\n",
    "X_train_dtm = cv.fit_transform(X_train)\n",
    "X_test_dtm = cv.transform(X_test)\n",
    "\n",
    "tfidf = TfidfTransformer()\n",
    "X_train_dtm = tfidf.fit_transform(X_train_dtm)\n",
    "X_test_dtm = tfidf.transform(X_test_dtm)\n",
    "@interact\n",
    "def try_penalties(penalty=penalty_options, alpha=alpha_options):\n",
    "    \n",
    "    clf = SGDClassifier(penalty=penalty, alpha=float(alpha), random_state=0)\n",
    "    clf.fit(X_train_dtm, y_train)\n",
    "    y_pred = clf.predict(X_test_dtm)\n",
    "    evaluate_model(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='2.2.3.3'></a>\n",
    "#### 2.2.3.3 alpha\n",
    "\n",
    "__alpha :__ float\n",
    "\n",
    "Constant that multiplies the regularization term. Defaults to 0.0001 Also used to compute learning_rate when set to ‘optimal’."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Example__\n",
    "\n",
    "We will plot a range of alpha values to see how they affect our test sample's accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# values = []\n",
    "cv = CountVectorizer()\n",
    "tfidf = TfidfTransformer()\n",
    "\n",
    "X_train_dtm = cv.fit_transform(X_train)\n",
    "X_test_dtm = cv.transform(X_test)\n",
    "X_train_dtm = tfidf.fit_transform(X_train_dtm)\n",
    "X_test_dtm = tfidf.transform(X_test_dtm)\n",
    "\n",
    "for i, alpha in enumerate(list(np.logspace(-6, -3))):\n",
    "    \n",
    "    clf = SGDClassifier(random_state=0, alpha=alpha, penalty='l1')\n",
    "    clf.fit(X_train_dtm, y_train)\n",
    "    y_pred = clf.predict(X_test_dtm)\n",
    "    plt.scatter(i, accuracy_score(y_test, y_pred))\n",
    "    plt.scatter(i, accuracy_score(y_train, clf.predict(X_train_dtm)), color='red')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='2.2.3.4'></a>\n",
    "#### 2.2.3.4 learning_rate\n",
    "\n",
    "__learning_rate :__ string, optional\n",
    "\n",
    "The learning rate schedule:\n",
    "  * ‘constant’: eta = eta0\n",
    "  * ‘optimal’: eta = 1.0 / (alpha * (t + t0)) [default]\n",
    "  * ‘invscaling’: eta = eta0 / pow(t, power_t)\n",
    "  \n",
    "where t0 is chosen by a heuristic proposed by Leon Bottou.\n",
    "\n",
    "__Example__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "learning_options = ['constant', 'optimal', 'invscaling']\n",
    "eta_options = [.0001, .001, .01, .1]\n",
    "cv = CountVectorizer()\n",
    "X_train_dtm = cv.fit_transform(X_train)\n",
    "X_test_dtm = cv.transform(X_test)\n",
    "\n",
    "tfidf = TfidfTransformer()\n",
    "X_train_dtm = tfidf.fit_transform(X_train_dtm)\n",
    "X_test_dtm = tfidf.transform(X_test_dtm)\n",
    "@interact\n",
    "def try_epsilon(learning_rate=learning_options, eta0=eta_options):\n",
    "    \n",
    "    clf = SGDClassifier(learning_rate=learning_rate, eta0=float(eta0), random_state=0)\n",
    "    clf.fit(X_train_dtm, y_train)\n",
    "    y_pred = clf.predict(X_test_dtm)\n",
    "    evaluate_model(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='2.2.4'></a>\n",
    "### 2.2.4 Getting best features out\n",
    "\n",
    "SGDClassifier makes it very easy to get the best features out of our model, which can be very helpful if we're trying to shrink our feature space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Simple model fit for SGDClassifier\n",
    "#----------------------------------\n",
    "cv = CountVectorizer()\n",
    "tfidf = TfidfTransformer()\n",
    "\n",
    "X_train_dtm = cv.fit_transform(X_train)\n",
    "X_test_dtm = cv.transform(X_test)\n",
    "\n",
    "X_train_dtm = tfidf.fit_transform(X_train_dtm)\n",
    "X_test_dtm = tfidf.transform(X_test_dtm)\n",
    "\n",
    "clf.fit(X_train_dtm, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_coef_df(sgd_clf, vect):\n",
    "    \n",
    "    sgd_clf.densify()\n",
    "    ndf = pd.DataFrame(clf.coef_.T, columns=train.target_names)\n",
    "    ndf['Token'] = cv.get_feature_names()\n",
    "    return ndf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "coef_df = get_coef_df(clf, cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "coef_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_most_important(ratio_df, num, return_tokens=False, ascending=False):\n",
    "    \n",
    "    list_tokens = []\n",
    "    for column in ratio_df.columns[:-1]:\n",
    "        ratio_df = ratio_df.sort_values(by=column, ascending=ascending)\n",
    "        list_tokens += ratio_df.iloc[:num]['Token'].tolist()\n",
    "        if return_tokens == False:\n",
    "            print(\"Token coefficients for {}:\\n\".format(column), ratio_df[['Token', column]][:num])\n",
    "    if return_tokens == True:\n",
    "        return list(set(list_tokens))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "get_most_important(coef_df, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Exercise:__\n",
    "\n",
    "  * 1) Get the 50 most important bi-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# %load exercise5.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_important_features(coef, feature_names, top_n=20, ax=None):\n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "    inds = np.argsort(coef)\n",
    "    low = inds[:top_n]\n",
    "    high = inds[-top_n:]\n",
    "    important = np.hstack([low, high])\n",
    "    myrange = range(len(important))\n",
    "\n",
    "    ax.bar(myrange, coef[important])\n",
    "    ax.set_xticks(myrange)\n",
    "    ax.set_xticklabels(feature_names[important], rotation=60, ha=\"right\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "names = pd.Series(cv.get_feature_names())\n",
    "plt.figure(figsize=(12,8))\n",
    "plot_important_features(coef_df['talk.politics.misc'], names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "top_words = get_most_important(coef_df, 100, return_tokens=True, ascending=True)\n",
    "bottom_words = get_most_important(coef_df, 100, return_tokens=True, ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "best_words = top_words + bottom_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(best_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(set(best_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cv = CountVectorizer(vocabulary=set(top_words))\n",
    "clf = SGDClassifier(random_state=0)\n",
    "tfidf = TfidfTransformer()\n",
    "\n",
    "X_train_dtm = cv.fit_transform(X_train)\n",
    "X_test_dtm = cv.transform(X_test)\n",
    "\n",
    "X_train_dtm = tfidf.fit_transform(X_train_dtm)\n",
    "X_test_dtm = tfidf.transform(X_test_dtm)\n",
    "\n",
    "clf.fit(X_train_dtm, y_train)\n",
    "y_pred = clf.predict(X_test_dtm)\n",
    "evaluate_model(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cv = CountVectorizer(stop_words='english', max_df=.4, max_features=700)\n",
    "clf = SGDClassifier(random_state=0)\n",
    "tfidf = TfidfTransformer()\n",
    "\n",
    "X_train_dtm = cv.fit_transform(X_train)\n",
    "X_test_dtm = cv.transform(X_test)\n",
    "\n",
    "X_train_dtm = tfidf.fit_transform(X_train_dtm)\n",
    "X_test_dtm = tfidf.transform(X_test_dtm)\n",
    "\n",
    "clf.fit(X_train_dtm, y_train)\n",
    "y_pred = clf.predict(X_test_dtm)\n",
    "evaluate_model(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "results = []\n",
    "start = 50#50\n",
    "end = 400#200\n",
    "intervals = np.linspace(start, end, num=10)\n",
    "for num in [int(x) for x in intervals]:\n",
    "    \n",
    "    best_tokens = get_most_important(coef_df, num, return_tokens=True, ascending=False)\n",
    "    best_tokens += get_most_important(coef_df, num, return_tokens=True, ascending=True)\n",
    "    \n",
    "    best_cv = CountVectorizer(vocabulary=set(best_tokens), ngram_range=(1,1))\n",
    "    best_tfidf = TfidfTransformer()\n",
    "    \n",
    "    best_train_dtm = best_cv.fit_transform(X_train)\n",
    "    \n",
    "    best_nb = MultinomialNB()\n",
    "    best_nb.fit(best_train_dtm, y_train)\n",
    "    best_nb_acc = accuracy_score(y_test, best_nb.predict(best_cv.transform(X_test)))\n",
    "    \n",
    "    best_sgd = SGDClassifier(random_state=0)\n",
    "    best_train_dtm = best_tfidf.fit_transform(best_train_dtm)\n",
    "    best_test_dtm = best_tfidf.transform(best_cv.transform(X_test))\n",
    "    best_sgd.fit(best_train_dtm, y_train)\n",
    "    \n",
    "    best_sgd_acc = accuracy_score(y_test, best_sgd.predict(best_test_dtm))\n",
    "                                 \n",
    "    \n",
    "    max_cv = CountVectorizer(max_features=len(set(best_tokens)), ngram_range=(1,1), stop_words='english', max_df=.2)\n",
    "    max_tfidf = TfidfTransformer()\n",
    "    max_train_dtm = max_cv.fit_transform(X_train)\n",
    "    \n",
    "    max_nb = MultinomialNB()\n",
    "    max_nb.fit(max_train_dtm, y_train)\n",
    "    max_nb_acc = accuracy_score(y_test, max_nb.predict(max_cv.transform(X_test)))\n",
    "                                  \n",
    "    max_sgd = SGDClassifier(random_state=0)\n",
    "    max_train_dtm = max_tfidf.fit_transform(max_train_dtm)\n",
    "    max_test_dtm = max_tfidf.transform(max_cv.transform(X_test))\n",
    "    max_sgd.fit(max_train_dtm, y_train)\n",
    "    max_sgd_acc = accuracy_score(y_test, max_sgd.predict(max_cv.transform(X_test)))\n",
    "    \n",
    "    results.append((len(set(best_tokens)), best_nb_acc, best_sgd_acc, max_nb_acc, max_sgd_acc))                            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(results, columns=['Num_Features', 'Best_NB', 'Best_SGD', 'Max_NB', 'Max_SGD'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,8))\n",
    "results_df.set_index('Num_Features').plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='3'></a>\n",
    "# 3 Tuning Models\n",
    "\n",
    "The models default parameters give us a good model, but if we want a great model we'll have to start turning the knobs and find our own optimal parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='3.1'></a>\n",
    "## 3.1 Pipeline\n",
    "\n",
    "Typing new parameters and keeping track of their results is a pain, lucky for us sklearn has a wonderful class called pipeline so we don't have to do that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='3.1.1'></a>\n",
    "### 3.1.1 Description\n",
    "\n",
    "__FROM SKLEARN [DOCUMENTATION](http://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html):__\n",
    "\n",
    "Pipeline of transforms with a final estimator.\n",
    "\n",
    "Sequentially apply a list of transforms and a final estimator. Intermediate steps of the pipeline must be ‘transforms’, that is, they must implement fit and transform methods. The final estimator only needs to implement fit.\n",
    "\n",
    "The purpose of the pipeline is to assemble several steps that can be cross-validated together while setting different parameters. For this, it enables setting parameters of the various steps using their names and the parameter name separated by a ‘\\_\\_’, as in the example below. A step’s estimator may be replaced entirely by setting the parameter with its name to another estimator, or a transformer removed by setting to None."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='3.1.2'></a>\n",
    "### 3.1.2 Example\n",
    "\n",
    "__1.__ We'll create a simple pipeline for SGDClassifier using the [make_pipeline](http://scikit-learn.org/stable/modules/generated/sklearn.pipeline.make_pipeline.html) function in sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pipe = make_pipeline(CountVectorizer(), TfidfTransformer(), SGDClassifier(random_state=0))\n",
    "\n",
    "pipe.fit(X_train, y_train)\n",
    "y_pred = pipe.predict(X_test)\n",
    "evaluate_model(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__2.__ We'll now build a pipeline without the make_pipeline helper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "        ('cv', CountVectorizer()),\n",
    "        ('tfidf', TfidfTransformer()),\n",
    "        ('sgd', SGDClassifier(random_state=0))\n",
    "    ])\n",
    "pipe.fit(X_train, y_train)\n",
    "y_pred = pipe.predict(X_test)\n",
    "evaluate_model(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='3.2'></a>\n",
    "## 3.2 Parameter Search\n",
    "\n",
    "The main reason these pipelines are used is to help people with parameter searching. There are two popular parameter search classes in sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='3.2.1'></a>\n",
    "### 3.2.1 GridSearchCV\n",
    "\n",
    "This is most well known searching technique in sklearn\n",
    "\n",
    "<a id='3.2.1.1'></a>\n",
    "#### 3.2.1.1 Description\n",
    "\n",
    "__FROM SKLEARN [DOCUMENTATION](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html):__\n",
    "\n",
    "Exhaustive search over specified parameter values for an estimator.\n",
    "\n",
    "Important members are fit, predict.\n",
    "\n",
    "GridSearchCV implements a “fit” and a “score” method. It also implements “predict”, “predict_proba”, “decision_function”, “transform” and “inverse_transform” if they are implemented in the estimator used.\n",
    "\n",
    "The parameters of the estimator used to apply these methods are optimized by cross-validated grid-search over a parameter grid.\n",
    "\n",
    "<a id='3.2.1.2'></a>\n",
    "#### 3.2.1.2 Example\n",
    "\n",
    "We'll show a small grid search for the optimal parameters for our multinomial naive bayes model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pipe = make_pipeline(CountVectorizer(), TfidfTransformer(), MultinomialNB())\n",
    "\n",
    "#These are the parameters we're searching over\n",
    "params = {\n",
    "    'countvectorizer__ngram_range':[(1,1), (1,2)],\n",
    "    #'countvectorizer__min_df':[2, 3, 4, 5],\n",
    "    #'countvectorizer__max_df':[.2, .4, .6, .8, 1.0],\n",
    "    'tfidftransformer__norm':['l2', None],\n",
    "    'tfidftransformer__use_idf':[True, False],\n",
    "    #'multinomialnb__alpha':[.001, .01, .1, 1]\n",
    "}\n",
    "grid = GridSearchCV(pipe, params, cv=5, verbose=True, n_jobs=-1)\n",
    "grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def evaluate_grid(_grid, metric='accurracy'):\n",
    "    print(\"Best CV Score: {:.2f}\".format(_grid.best_score_))\n",
    "    print(\"Best parameters: {}\".format(_grid.best_params_))\n",
    "    y_pred = _grid.predict(X_test)\n",
    "    print(\"\\taccuracy score on test: {}\".format(accuracy_score(y_test, y_pred)))\n",
    "    print(\"Classification report for test sample held out: \\n{}\".format(classification_report(y_test, y_pred)))\n",
    "    print(\"Confusion Matrix for test: \\n{}\".format(confusion_matrix(y_test, y_pred)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scores for each cross-validation are saved inside GridSearchCV object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "results = pd.DataFrame(grid.cv_results_).fillna('None')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "res_pivot = results.pivot_table(values=['mean_test_score', 'mean_train_score'],\n",
    "                                index=[\"param_countvectorizer__ngram_range\", \"param_tfidftransformer__use_idf\",\n",
    "                                       \"param_tfidftransformer__norm\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "res_pivot.mean_test_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='3.2.1.3'></a>\n",
    "#### 3.2.1.3 Parameters\n",
    "\n",
    "GridSearchCV has some very important parameters that will save you time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__scoring :__ string, callable or None, default=None\n",
    "\n",
    "A string (see model evaluation documentation) or a scorer callable object / function with signature scorer(estimator, X, y). If None, the score method of the estimator is used.\n",
    "\n",
    "__[LIST OF SCORING FUNCTIONS](http://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter):__\n",
    "\n",
    "<img src=\"./sklearn_scoring_table.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "politics = y_train[y_train == 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(politics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "politics.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_sample_df = X_train.drop(politics.index[:300])\n",
    "y_sample_df = y_train.drop(politics.index[:300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(X_sample_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Counter(y_sample_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample_X_train, sample_X_test, sample_y_train, sample_y_test = train_test_split(X_sample_df,\n",
    "                                                                                y_sample_df, test_size=.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "scoring = ['accuracy', 'neg_log_loss', 'f1_macro']\n",
    "\n",
    "@interact\n",
    "def try_score(score=scoring):\n",
    "    pipe = make_pipeline(CountVectorizer(max_features=2000), TfidfTransformer(), MultinomialNB())\n",
    "\n",
    "    #These are the parameters we're searching over\n",
    "    params = {\n",
    "        #'countvectorizer__ngram_range':[(1,1), (1,2)],\n",
    "        #'countvectorizer__min_df':[2, 3, 4, 5],\n",
    "        #'countvectorizer__max_df':[.2, .4, .6, .8, 1.0],\n",
    "        'tfidftransformer__norm':['l1', 'l2', None],\n",
    "        'tfidftransformer__use_idf':[True, False],\n",
    "        'multinomialnb__alpha':[.001, .01, .1, 1]\n",
    "    }\n",
    "    grid = GridSearchCV(pipe, params, cv=3, verbose=True, n_jobs=-1, scoring=score)\n",
    "    grid.fit(sample_X_train, sample_y_train)\n",
    "    \n",
    "    \n",
    "    print(\"Best CV Score: {:.2f}\".format(grid.best_score_))\n",
    "    print(\"Best parameters: {}\".format(grid.best_params_))\n",
    "    sample_y_pred = grid.predict(sample_X_test)\n",
    "    print(\"\\taccuracy score on test: {}\".format(accuracy_score(sample_y_test, sample_y_pred)))\n",
    "    print(\"Classification report for test sample held out: \\n{}\".format(classification_report(sample_y_test, sample_y_pred)))\n",
    "    print(\"Confusion Matrix for test: \\n{}\".format(confusion_matrix(sample_y_test, sample_y_pred)))\n",
    "    #evaluate_grid(grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__n_jobs :__ int, default=1\n",
    "\n",
    "Number of jobs to run in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "t0 = time()\n",
    "n_jobs = 1\n",
    "pipe = make_pipeline(CountVectorizer(), MultinomialNB())\n",
    "\n",
    "#These are the parameters we're searching over\n",
    "params = {\n",
    "    'countvectorizer__ngram_range':[(1,1), (1,2)],\n",
    "    'countvectorizer__stop_words':['english', None],\n",
    "    #'countvectorizer__min_df':[2, 3, 4, 5],\n",
    "    #'countvectorizer__max_df':[.2, .4, .6, .8, 1.0],\n",
    "}\n",
    "grid = GridSearchCV(pipe, params, cv=5, verbose=True, n_jobs=n_jobs)\n",
    "grid.fit(X_train, y_train)\n",
    "print('Took {} seconds with n_jobs={}'.format(time()-t0, n_jobs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "t0 = time()\n",
    "n_jobs = -1\n",
    "pipe = make_pipeline(CountVectorizer(), MultinomialNB())\n",
    "\n",
    "#These are the parameters we're searching over\n",
    "params = {\n",
    "    'countvectorizer__ngram_range':[(1,1), (1,2)],\n",
    "    'countvectorizer__stop_words':['english', None],\n",
    "    #'countvectorizer__min_df':[2, 3, 4, 5],\n",
    "    #'countvectorizer__max_df':[.2, .4, .6, .8, 1.0],\n",
    "}\n",
    "grid = GridSearchCV(pipe, params, cv=5, verbose=True, n_jobs=n_jobs)\n",
    "grid.fit(X_train, y_train)\n",
    "print('Took {} seconds with n_jobs={}'.format(time()-t0, n_jobs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__cv :__ int, cross-validation generator or an iterable, optional\n",
    "\n",
    "Determines the cross-validation splitting strategy. Possible inputs for cv are:\n",
    "\n",
    "  * None, to use the default 3-fold cross validation,\n",
    "\n",
    "  * integer, to specify the number of folds in a (Stratified)KFold,\n",
    "\n",
    "  * An object to be used as a cross-validation generator.\n",
    "\n",
    "  * An iterable yielding train, test splits.\n",
    "\n",
    "For integer/None inputs, if the estimator is a classifier and y is either binary or multiclass, [StratifiedKFold](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedKFold.html#sklearn.model_selection.StratifiedKFold) is used. In all other cases, KFold is used.\n",
    "\n",
    "Refer [User Guide](http://scikit-learn.org/stable/modules/cross_validation.html#cross-validation) for the various cross-validation strategies that can be used here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "shufflesplit = ShuffleSplit(n_splits=5, test_size=0.25, random_state=0)\n",
    "pipe = make_pipeline(CountVectorizer(), MultinomialNB())\n",
    "\n",
    "#These are the parameters we're searching over\n",
    "params = {\n",
    "    'countvectorizer__ngram_range':[(1,1), (1,2)],\n",
    "    #'countvectorizer__min_df':[2, 3, 4, 5],\n",
    "    #'countvectorizer__max_df':[.2, .4, .6, .8, 1.0],\n",
    "    #'multinomialnb__alpha':[.001, .01, .1, 1]\n",
    "}\n",
    "grid = GridSearchCV(pipe, params, cv=shufflesplit, verbose=True, n_jobs=-1)\n",
    "grid.fit(X_train, y_train)\n",
    "evaluate_grid(grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='3.2.2'></a>\n",
    "### 3.2.2 RandomizedSearchCV\n",
    "\n",
    "__FROM SKLEARN [DOCUMENTATION](http://scikit-learn.org/stable/modules/grid_search.html#randomized-parameter-search):__\n",
    "\n",
    "While using a grid of parameter settings is currently the most widely used method for parameter optimization, other search methods have more favourable properties. RandomizedSearchCV implements a randomized search over parameters, where each setting is sampled from a distribution over possible parameter values. This has two main benefits over an exhaustive search:\n",
    "\n",
    "  * A budget can be chosen independent of the number of parameters and possible values.\n",
    "  * Adding parameters that do not influence the performance does not decrease efficiency.\n",
    "  \n",
    "Specifying how parameters should be sampled is done using a dictionary, very similar to specifying parameters for GridSearchCV.\n",
    "\n",
    "Additionally, a computation budget, being the number of sampled candidates or sampling iterations, is specified using the n_iter parameter. For each parameter, either a distribution over possible values or a list of discrete choices (which will be sampled uniformly) can be specified:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='3.2.2.1'></a>\n",
    "#### 3.2.2.1 Description\n",
    "\n",
    "__FROM SKLEARN [DOCUMENTATION](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html):__\n",
    "\n",
    "Randomized search on hyper parameters.\n",
    "\n",
    "RandomizedSearchCV implements a “fit” and a “score” method. It also implements “predict”, “predict_proba”, “decision_function”, “transform” and “inverse_transform” if they are implemented in the estimator used.\n",
    "\n",
    "The parameters of the estimator used to apply these methods are optimized by cross-validated search over parameter settings.\n",
    "In contrast to GridSearchCV, not all parameter values are tried out, but rather a fixed number of parameter settings is sampled from the specified distributions. The number of parameter settings that are tried is given by n_iter.\n",
    "\n",
    "If all parameters are presented as a list, sampling without replacement is performed. If at least one parameter is given as a distribution, sampling with replacement is used. It is highly recommended to use continuous distributions for continuous parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='3.2.2.2'></a>\n",
    "#### 3.2.2.2 Example\n",
    "\n",
    "RandomizedSearchCV is very similar to GridSearchCV with the only difference being n_iter parameter which will allow us to select from a bigger parameter grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pipe = make_pipeline(CountVectorizer(), MultinomialNB())\n",
    "\n",
    "#These are the parameters we're searching over\n",
    "params = {\n",
    "    'countvectorizer__ngram_range':[(1,1), (1,2), (1,3)],\n",
    "    'countvectorizer__min_df':[0,2,3,4,5,6,7,8,9,10],\n",
    "    'countvectorizer__max_df':[.1, .2, .3, .4, .5, .6, .7, .8, .9, 1.0],\n",
    "    'multinomialnb__alpha':[.00001, .0001, .001, .01, .1, 1]\n",
    "}\n",
    "grid = RandomizedSearchCV(pipe, params, cv=5, verbose=True, n_iter=10, n_jobs=-1)\n",
    "grid.fit(X_train, y_train)\n",
    "evaluate_grid(grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "__EXERCISE:__\n",
    "\n",
    "  * 1) Create a pipeline and Grid/Randomized search CV for optimal parameters for SGDClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='4'></a>\n",
    "# 4 Increasing Performance\n",
    "\n",
    "There are many ways to increase model performance. The best way is typically feature engineering, but we're just covering text mining here\n",
    "\n",
    "<a id='4.1'></a>\n",
    "## 4.1 Stemming\n",
    "\n",
    "One thing we can do is stem our text to shrink our feature space. This sometimes gives a better model but not always, sometimes the different classes with use different suffixes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='4.1.1'></a>\n",
    "### 4.1.1 Types\n",
    "\n",
    "We will only focus on three of the more popular stemmers\n",
    "\n",
    "### Porter\n",
    "\n",
    "This is the most common stemmer\n",
    "\n",
    "__EXAMPLE__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p_stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "p_stemmer.stem('player')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lancaster\n",
    "\n",
    "This is a more aggresive stemmer, meaning it will chop off suffixes more frequently\n",
    "\n",
    "__EXAMPLE__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "l_stemmer = LancasterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "l_stemmer.stem('player')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Snowball \n",
    "\n",
    "Adapted version of Porter, must specify the language\n",
    "\n",
    "__EXAMPLE__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s_stemmer = SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "s_stemmer.stem('generously')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "p_stemmer.stem('generously')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "l_stemmer.stem('generously')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='4.1.2'></a>\n",
    "### 4.1.2 Stemming our Text\n",
    "\n",
    "Before we begin stemming our words we need to make sure stop_words aren't being stemmed because they won't be picked up by our CountVectorizer if the word is different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stop_words = nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stop_words[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for word in stop_words[50:100]:\n",
    "    \n",
    "    if len(word) > 5:\n",
    "        print('\\nStop word: {}'.format(word))\n",
    "        print('\\tPorter: {}\\n\\tLancaster: {}\\n\\tSnowball: {}'.format(p_stemmer.stem(word), l_stemmer.stem(word),\n",
    "                                                                          s_stemmer.stem(word)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will then have to ignore these words when stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "token_regex = re.compile(r'\\b\\w\\w+\\b')\n",
    "def stem_column(col, stemmer, stop_words):\n",
    "    return col.apply(lambda x: ' '.join([stemmer.stem(token) if token not in stop_words else token\n",
    "                                         for token in token_regex.findall(x)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_df = df['Text'].head(10)\n",
    "snowball_text = stem_column(test_df, s_stemmer, stop_words)\n",
    "porter_text = stem_column(test_df, p_stemmer, stop_words)\n",
    "lancaster_text = stem_column(test_df, l_stemmer, stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "i = 4\n",
    "print('Original: \\n{}'.format(test_df.iloc[i]))\n",
    "print('\\nSnowball Stemmed: \\n\\n{}'.format(snowball_text.iloc[i]))\n",
    "print('\\nPorter Stemmed: \\n\\n{}'.format(snowball_text.iloc[i]))\n",
    "print('\\nLancaster Stemmed: \\n\\n{}'.format(snowball_text.iloc[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df['snowball_text'] = stem_column(df['Text'], s_stemmer, stop_words)\n",
    "df['porter_text'] = porter_text = stem_column(df['Text'], p_stemmer, stop_words)\n",
    "df['lancaster_text'] = lancaster_text = stem_column(df['Text'], l_stemmer, stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for col in ['Text', 'snowball_text', 'porter_text', 'lancaster_text']:\n",
    "    cv = CountVectorizer(stop_words='english')\n",
    "    dtm = cv.fit_transform(df[col])\n",
    "    print('There are {} tokens for column: {}'.format(dtm.shape[1], col))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='4.2'></a>\n",
    "## 4.2 Custom Transformer (Advanced)\n",
    "\n",
    "We know pipelines take in transformers, but until now we've been using sklearns transformers. Most of the time this will be fine, but if you know how to create your own transformer you'll have much greater power.\n",
    "\n",
    "I already created one and saved it in file column_selector.py, this is necessary for GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from column_selector import ColSelector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "??ColSelector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a very simple transformer, that just grabs what ever column from the data set you specify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "col = ColSelector(key='Text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "col.fit_transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('selector', ColSelector(key='Text')),\n",
    "    ('cv', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('sgd', SGDClassifier())\n",
    "    ])\n",
    "\n",
    "params = {\n",
    "    'selector__key':['Text', 'snowball_text', 'porter_text', 'lancaster_text'],\n",
    "    'cv__stop_words':['english'],\n",
    "    'cv__ngram_range':[(1,1), (1,2), (1,3)],\n",
    "    'cv__min_df':[2, 3, 4],\n",
    "    'cv__max_df':[.5, .6, .7, .8, .9, 1.0],\n",
    "    'tfidf__use_idf':[True],\n",
    "    'tfidf__norm':['l1', 'l2'],\n",
    "    'sgd__alpha':np.logspace(-7, -2, 20),\n",
    "    'sgd__loss':['hinge', 'modified_huber'],\n",
    "    'sgd__epsilon':np.logspace(-6, 0, 10)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train2, X_test2, y_train2, y_test2 = train_test_split(df, train.target, test_size=.2, random_state=0)\n",
    "n_iter = 200\n",
    "t0 = time()\n",
    "\n",
    "stemming_grid = RandomizedSearchCV(pipeline, params, cv=5, n_iter=n_iter, n_jobs=-1, verbose=True)\n",
    "stemming_grid.fit(X_train2, y_train2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Best CV Score: {:.2f}\".format(stemming_grid.best_score_))\n",
    "print(\"Best parameters: {}\".format(stemming_grid.best_params_))\n",
    "y_pred = stemming_grid.predict(X_test2)\n",
    "print(\"Classification report for test sample held out: \\n{}\".format(classification_report(y_test2, y_pred)))\n",
    "print(\"Confusion Matrix for test: \\n{}\".format(confusion_matrix(y_test2, y_pred)))\n",
    "print('Ran {} iterations in {} seconds'.format(n_iter, time()-t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "results = pd.DataFrame(stemming_grid.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "results.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "results.groupby(by='param_selector__key')['mean_test_score'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "results = results.fillna('None')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "param_cols = [col for col in results.columns if col.startswith('param_')]\n",
    "@interact\n",
    "def compare_parameter(col=param_cols):\n",
    "    print(results.groupby(by=col)['mean_test_score'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "__EXERCISE__\n",
    "\n",
    "  * 1) Write a text classification pipeline to classify yelp reviews between 1-5 stars.\n",
    "  * 2) Find a good set of parameters using grid search.\n",
    "  * 3) Evaluate the performance on a held out test set.\n",
    "  \n",
    "  __Extra:__ Find a new sklearn classifier and build a pipeline with its parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/yelp.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {
    "0c8528fed51f453f9b193fb9ddd1f43e": {
     "views": [
      {
       "cell_index": 156
      }
     ]
    },
    "1ed0e6f810df4b3ea2e8d6963460b43e": {
     "views": [
      {
       "cell_index": 259
      }
     ]
    },
    "3c832e8c709d48c79e8a36f67547b77b": {
     "views": [
      {
       "cell_index": 161
      }
     ]
    },
    "4c8d85117b0c4c16962c59045ae5c243": {
     "views": [
      {
       "cell_index": 206
      }
     ]
    },
    "58d22507a5e5431c97f092fcfaf8e21b": {
     "views": [
      {
       "cell_index": 134
      }
     ]
    },
    "9937b600396d4789813cf8b2f0059609": {
     "views": [
      {
       "cell_index": 120
      }
     ]
    },
    "9af8de49bdca40f1848f8d142493f1fb": {
     "views": [
      {
       "cell_index": 154
      }
     ]
    },
    "feaf0741838246f9a7f9de466159aa1a": {
     "views": [
      {
       "cell_index": 136
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
